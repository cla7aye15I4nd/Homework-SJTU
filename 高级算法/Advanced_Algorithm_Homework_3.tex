% This is a template for lecture notes.
\documentclass{article}
\usepackage[UTF8]{ctex}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{bm}
\usepackage{tcolorbox}
\CTEXoptions[today=old]

%Some commonly used notations
%\geometry{a4paper,bottom = 3cm,left = 3cm, right = 3cm}

%for reference
\usepackage{hyperref}
\usepackage[capitalise]{cleveref}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{question}[theorem]{Question}
\newtheorem{answer}[theorem]{Answer}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{example}[theorem]{Example}
\newenvironment{solution}{\noindent \textit{proof:}}{$\Box$}
\newtheorem{observation}[theorem]{Observation}

%to use newcommand for convenience
\newcommand\field{\mathbb{F}}
\newcommand\Real{\mathbb{R}}
\newcommand\Q{\mathbb{Q}}
\newcommand\Z{\mathbb{Z}}
\newcommand\complex{\mathbb{C}}

%this is how we define operators.
\DeclareMathOperator{\rank}{rank} % rank

\title{Advanced Algorithm Homework 3}
\author{于峥 518030910437}
\date{\today}

\begin{document}
    \maketitle
    
\begin{problem}
\end{problem}
\begin{solution}
    Let $X = \sum_{i=1}^np_ib_i(t)$ be the effect of all noise.
    If $b(t) = -1$, the probability that receiver makes an error is
    \begin{align*}
    \mathrm{Pr}(s(t) \geq 0) 
    &= \mathrm{Pr}\left(X \geq 1\right) \\
    &= \mathrm{Pr(e^{tX} \geq e^t})  \\
    &\leq \frac {\mathrm{E}(e^{tX})} {e^t} 
    = e^{-t} \prod_{i=1}^n \mathrm{E}(e^{tp_ib_i}) \\
    &= e^{-t} \prod_{i=1}^n \frac 1 2 (e^{tp_i} + e^{-tp_i}) \\
    &\leq e^{-t} \prod_{i=1}^n e^{t^2p_i^2/2} \\
    &= \exp(\frac {t^2} 2\sum_{i=1}^n p_i^2 - t)
    \end{align*}
    
    So we let $t = \frac {1} {\sum_{i=1}^np_i^2}$, so $\mathrm{Pr}(s(t)\geq 0) \leq \exp(-\frac 1 {2\sum_{i=1}^np_i^2})$, and if $b(t) = +1$ is symmetrical.
\end{solution}

\begin{problem} 
\end{problem}
\begin{solution}
    Consider we repeating the algorithm $q(n)$ times. We define the $X_i$ is the $i$-th output, let
    $\mathrm{Pr}(X_i=1) = \frac 1 2 + \frac 1 {p(n)}$,
    $\mathrm{Pr}(X_i=0) = \frac 1 2 - \frac 1 {p(n)}$, $X = \sum_i X_i$ Then the error probability is
    $$
        \mathrm{Pr}\left(X \leq \frac {q(n)} 2\right) = \mathrm{Pr}\left(X \leq \mu \left (1-\frac 2 {p(n)+2}\right)\right) ~~ \mu = q(n) \left(\frac 1 2 + \frac 1 {p(n)}\right)
    $$
    
    Note that $0 < \frac 2 {p(n)+2} < 1$, so we can use the Multiplicative Chernoff bound
    $$
    \mathrm{Pr}\left(X \leq \mu \left (1-\frac 2 {p(n)+2}\right)\right)
    \leq \mathrm{exp} \left( -\frac \mu 2 \left(\frac 2 {p(n)+2} \right)^2 \right)
    = \exp\left(-\frac {q(n)} {p(n)(p(n)+2)}\right)
    $$
    
    We can simply set $q(n) = np(n)(p(n)+2)$, the error probability is less that
    $2^{-n}$.
\end{solution}

\begin{problem}
\end{problem}
\begin{solution}

(1) We can expand the chernoff bound by Taylor expansion
$$
\frac {\mathrm{E}[e^{t|X|}]} {e^{t\delta}} = \frac {\sum_{i=0}^\infty \mathrm{E}\left(\frac {t^i|X|^i} {i!}\right)} {\sum_{i=0}^\infty \frac {(t\delta)^i} {i!}}
$$
and let 
$$
    b = \inf_k \frac {\mathrm{E}[|X|^k]} {\delta^k}
$$
then we have for any $i$,
$$
\frac {\mathrm{E}\left(\frac {t^i|X|^i} {i!}\right)} {\frac {(t\delta)^i}{i!}} =  
\frac {\mathrm{E}[|X|^i]} {\delta^i} \geq b
$$
Hence, we get $\frac {\mathrm{E}[e^{t|X|}]} {e^{t\delta}} \geq b$ by the suger water inequality, such $k$ that the kth moment bound is stronger than the Chernoff bound can be found.

(2) To find the specific $k$ is not easy, we need to solve a complex transcendental equation to get it. Although the chernoff bound is not tighter than 
k-momnet bound, it is convenient to calculate.
\end{solution}

\begin{problem}
\end{problem}
\begin{solution}
    We have known that $R(T) \leq \sum_{i=1}^k (L\Delta_i+T\Delta_i\exp(-\frac {L\Delta_i^2} {2}))$ in class. In this problem, 
    $$
    R(T) \leq L\Delta_2+T\Delta_2\exp\left(-\frac {L\Delta_2^2} {2}\right)
    $$
    let $f(\Delta_2, L) = L\Delta_2+T\Delta_2\exp\left(-\frac {L\Delta_2^2} {2}\right)$, then we just need to calculate 
    $$
    \max_{\Delta_2}\min_{L} f(\Delta_2,L)
    $$
    Compute the min point 
    $$
    \frac {\partial f} {\partial L} = \Delta_2 - 
    \frac {T\Delta_2^3} {2} \exp\left(-\frac {L\Delta_2^2} {2}\right) 
    ~~~ L^* = \frac {2} {\Delta_2^2}\ln \frac {T\Delta_2} {2}
    $$
    Then let $g(\Delta_2) = \frac {2} {\Delta^2}\ln \frac {T\Delta_2^2} {2} + \frac {2} {\Delta_2}$, we need find the max point,
    $$
    \frac {\partial g} {\partial \Delta_2} = -\frac {2}{\Delta_2^2}\ln \frac {T\Delta_2^2} {2} + \frac {2} {\Delta_2^2} ~~~ 
    \Delta_2^*  = \sqrt{\frac {2e} {T}}. 
    $$
    Finally we get $g(\sqrt{\frac {2e} {T}}) = 2\sqrt{\frac {2T}{e}} = O(\sqrt{T})$.
    
\end{solution}

\end{document} 
